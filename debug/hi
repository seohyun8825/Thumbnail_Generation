x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
noise_pred min, max: -3.18359375 3.783203125
  5%|█████▎                                                                                                     | 1/20 [00:01<00:25,  1.36s/it]
Step 1, timestep 962:
latent_model_input_user min, max: -4.58203125 4.890625
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 1024, 1280])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
noise_pred min, max: -4.2578125 3.822265625
 10%|██████████▋                                                                                                | 2/20 [00:02<00:23,  1.33s/it]
Step 2, timestep 921:
latent_model_input_user min, max: -5.0703125 4.55859375
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 4096, 640])
x shape torch.Size([1, 1024, 1280])
 10%|██████████▋                                                                                                | 2/20 [00:02<00:24,  1.36s/it]
Traceback (most recent call last):
  File "/home/kubig/Thumbnail_Image_Generation/Generate.py", line 33, in <module>
    images = pipe(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/kubig/Thumbnail_Image_Generation/model/Modified_xl.py", line 1435, in __call__
    noise_pred = self.unet(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/diffusers/models/unets/unet_2d_condition.py", line 1216, in forward
    sample, res_samples = downsample_block(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/diffusers/models/unets/unet_2d_blocks.py", line 1279, in forward
    hidden_states = attn(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/diffusers/models/transformers/transformer_2d.py", line 397, in forward
    hidden_states = block(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/diffusers/models/attention.py", line 366, in forward
    attn_output = self.attn2(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/kubig/Thumbnail_Image_Generation/model/cross_attention.py", line 241, in forward
    opx = matsepcalc(px, conp, user_latents, divide=2)
  File "/home/kubig/Thumbnail_Image_Generation/model/cross_attention.py", line 175, in matsepcalc
    out = adjust_latent_to_x(out, x.shape, x.device, x.dtype)
  File "/home/kubig/Thumbnail_Image_Generation/model/cross_attention.py", line 44, in adjust_latent_to_x
    embedding_resize = nn.Linear(curr_embedding_dim, target_embedding_dim).to(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
KeyboardInterrupt

(RPG) root@C.13784856:/home/kubig/Thumbnail_Image_Generation$ python Generate.py
Loading pipeline components...: 100%|████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  6.31it/s]
waiting for GPT-4 response
Key Entities Identification:
The caption indicates "four snapshots of a dog." The key entity is a dog, which implies we may need to envision varied aspects or settings for
each snapshot.

1) Snapshot 1: The dog's head, possibly showcasing expressive eyes, ears, and a playful expression.
2) Snapshot 2: The dog's body, highlighting its posture, fur texture, and any distinct markings.
3) Snapshot 3: An action or playful moment, perhaps catching the dog running or jumping.
4) Snapshot 4: A quiet or resting scene, capturing the dog lying down or relaxing.

Plan the Structure Split for the Image:

a. Rows
Since there are four key snapshots or perspectives of the dog, we should split the image into multiple snapshots within a single row for this o
rderly presentation.

Row0 (height=1): Occupies the entire image height, dedicated to the various snapshots of the dog.

b. Regions within rows
Divide the row into four equal regions, each depicting a different snapshot.

Region0 (Row0, width=0.25): Close-up of the dog's head, focusing on its expressive eyes and attentive ears, highlighting its personality.
Region1 (Row0, width=0.25): Full body view, showing the dog's posture and any distinct fur patterns, possibly against a simple backdrop.
Region2 (Row0, width=0.25): Action shot, perhaps of the dog mid-jump or in a playful stance, with motion blur to convey movement.
Region3 (Row0, width=0.25): Tranquil scene, portraying the dog lying down, basking in the sun, or nestled in a cozy spot.

c. Overall Ratio:
Since there is only one row, we omit the row ratio and directly provide the widths of the regions within the row:

Final split ratio: 0.25,0.25,0.25,0.25

Regional Prompt:
Close-up of the dog's head, focusing on expressive eyes and attentive ears, showcasing a hint of playfulness in its gaze. BREAK Full body view,
 capturing the elegant posture and unique fur markings of the dog, positioned against a contrasting backdrop. BREAK Action shot of the dog mid-
leap, ears flying back, illustrating its playful energy with a blur of motion in its surroundings. BREAK Tranquil scene, depicting the dog lyin
g down, basking in a warm spot, exuding a sense of peace and contentment.
Final split ratio: 0.25,0.25,0.25,0.25
Regional Prompt not found.
Traceback (most recent call last):
  File "/home/kubig/Thumbnail_Image_Generation/Generate.py", line 20, in <module>
    para_dict = GPT4(prompt,key=api_key)
  File "/home/kubig/Thumbnail_Image_Generation/model/mllm.py", line 43, in GPT4
    return get_params_dict(text)
  File "/home/kubig/Thumbnail_Image_Generation/model/mllm.py", line 83, in get_params_dict
    image_region_dict = {'Final split ratio': final_split_ratio, 'Regional Prompt': regional_prompt}
UnboundLocalError: local variable 'regional_prompt' referenced before assignment
(RPG) root@C.13784856:/home/kubig/Thumbnail_Image_Generation$ python Generate.py
Loading pipeline components...: 100%|████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  6.11it/s]
waiting for GPT-4 response
Key entities identification:
The caption suggests four snapshots of a dog. Therefore, we can identify one key entity — a dog — but it's represented four times, implying fou
r different images or scenes. Each image can be imagined to depict distinct attributes or actions of the dog.

1. Dog Snapshot 1: Perhaps capturing the dog sitting attentively with perked ears.
2. Dog Snapshot 2: The dog might be caught mid-action, jumping or running.
3. Dog Snapshot 3: A close-up of the dog's face, showing its expressive eyes and playful demeanor.
4. Dog Snapshot 4: The dog lying down, relaxed, with its tail wagging.

Plan the structure split for the image:
Since we have four snapshots, we will split the image into four equal regions, each containing one snapshot.

a. Rows
We can consider a 2x2 grid, dividing the image into two rows and two columns.

Row0 (height=0.5): This row will occupy the top half of the image, containing the first two snapshots of the dog.

Row1 (height=0.5): This row will occupy the bottom half of the image, containing the last two snapshots of the dog.

b. Regions within rows
For each row, we will have two regions, representing the different snapshots:

Region0 (Row0, width=0.5): Depicts the dog sitting attentively, ears perked up, set against a simple background to highlight its posture.

Region1 (Row0, width=0.5): Shows the dog in motion, perhaps caught mid-leap, with blurred surroundings to emphasize the sense of movement.

Region2 (Row1, width=0.5): Features a close-up of the dog's face, capturing the sparkle in its eyes and the curious tilt of its head.

Region3 (Row1, width=0.5): Presents the dog lying down, relaxed, with a content expression and a slightly wagging tail, immersed in a cozy sett
ing.

c. Overall ratio:
Row0_height,Row0_region0_width,Row0_region1_width;Row1_height,Row1_region2_width,Row1_region3_width
Final split ratio: 0.5,0.5,0.5;0.5,0.5,0.5

Regional Prompt: Dog sitting attentively with perked ears, in a serene setting. BREAK Dog captured mid-action, jumping playfully with blurred b
ackground motion. BREAK Close-up of dog's expressive face and playful demeanor. BREAK Dog lying down, relaxed with a wagging tail in a cozy amb
iance.
Final split ratio: 0.5,0.5,0.5;0.5,0.5,0.5
Regional Prompt: Dog sitting attentively with perked ears, in a serene setting. BREAK Dog captured mid-action, jumping playfully with blurred b
ackground motion. BREAK Close-up of dog's expressive face and playful demeanor. BREAK Dog lying down, relaxed with a wagging tail in a cozy amb
iance.
[DEBUG] Preparing to call the pipeline...
[DEBUG] Checking user_images at start of __call__...
[DEBUG] user_images 내용: ['user_img/a.png', 'user_img/b.png', 'user_img/happy_sitting.png']
[DEBUG] split_ratio initialized: 0.5,0.5,0.5;0.5,0.5,0.5
[DEBUG] split_ratio2: [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]
[DEBUG] Flattened split_ratio: [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]
[DEBUG] Total Rows: 2
[DEBUG] Regions per Row: [3, 3]
[DEBUG] Total Region Count: 6
[DEBUG] Insufficient user images. Provided 3 but 6 regions are expected.
[DEBUG] Missing regions will be generated using the original prompt.
[DEBUG] Converting image user_img/a.png to latent...
[DEBUG] Successfully converted image user_img/a.png: latent.shape=torch.Size([1, 4096, 640])
[DEBUG] Converting image user_img/b.png to latent...
[DEBUG] Successfully converted image user_img/b.png: latent.shape=torch.Size([1, 4096, 640])
[DEBUG] Converting image user_img/happy_sitting.png to latent...
[DEBUG] Successfully converted image user_img/happy_sitting.png: latent.shape=torch.Size([1, 4096, 640])
[INFO] Generated 3 user latents out of 3 user images.
[INFO] 사용자 이미지 부족: 3개 중 6개 Region 필요.
[DEBUG] user_latents processed: [torch.Size([1, 4096, 640]), torch.Size([1, 4096, 640]), torch.Size([1, 4096, 640])]
[DEBUG] Initial split_ratio: [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]
[DEBUG] split_ratio2 before ratiosdealer: [[0.5, 0.5], [0.5, 0.5]]
[DEBUG] split_ratio2r before ratiosdealer: [0.5, 0.5]
'[[0.5' is not a number, converted to 1
' 0.5]' is not a number, converted to 1
' [0.5' is not a number, converted to 1
' 0.5]]' is not a number, converted to 1
split_ratio2r [1, 1]
split_ratio2 [[0.5, 1], [0.5]]
[[0.5, 0.5], [0.5]]
4
  0%|                                                                                                                   | 0/20 [00:00<?, ?it/s]
Step 0, timestep 999:
latent_model_input_user min, max: -2.466796875 2.3203125
noise_pred min, max: -2.36328125 2.404296875
  5%|█████▎                                                                                                     | 1/20 [00:01<00:25,  1.35s/it]
Step 1, timestep 962:
latent_model_input_user min, max: -3.97265625 4.64453125
noise_pred min, max: -3.29296875 3.349609375
 10%|██████████▋                                                                                                | 2/20 [00:02<00:23,  1.32s/it]
Step 2, timestep 921:
latent_model_input_user min, max: -4.921875 4.296875
noise_pred min, max: -3.505859375 3.39453125
 15%|████████████████                                                                                           | 3/20 [00:03<00:22,  1.31s/it]
Step 3, timestep 876:
latent_model_input_user min, max: -5.16796875 4.7734375
noise_pred min, max: -4.43359375 4.23828125
 20%|█████████████████████▍                                                                                     | 4/20 [00:05<00:20,  1.31s/it]
Step 4, timestep 827:
latent_model_input_user min, max: -6.7109375 7.125
noise_pred min, max: -5.0390625 4.453125
 25%|██████████████████████████▊                                                                                | 5/20 [00:06<00:19,  1.31s/it]
Step 5, timestep 772:
latent_model_input_user min, max: -7.19140625 6.25
noise_pred min, max: -5.34375 4.9609375
 30%|████████████████████████████████                                                                           | 6/20 [00:07<00:18,  1.30s/it]
Step 6, timestep 710:
latent_model_input_user min, max: -6.4765625 5.63671875
noise_pred min, max: -4.8671875 4.4765625
 35%|█████████████████████████████████████▍                                                                     | 7/20 [00:09<00:17,  1.31s/it]
Step 7, timestep 640:
latent_model_input_user min, max: -5.82421875 7.359375
noise_pred min, max: -4.68359375 5.875
 40%|██████████████████████████████████████████▊                                                                | 8/20 [00:10<00:15,  1.30s/it]
Step 8, timestep 561:
latent_model_input_user min, max: -8.984375 8.484375
noise_pred min, max: -6.67578125 4.8515625
 45%|████████████████████████████████████████████████▏                                                          | 9/20 [00:11<00:14,  1.30s/it]
Step 9, timestep 474:
latent_model_input_user min, max: -6.984375 6.640625
noise_pred min, max: -5.58203125 5.359375
 50%|█████████████████████████████████████████████████████                                                     | 10/20 [00:13<00:13,  1.30s/it]
Step 10, timestep 380:
latent_model_input_user min, max: -9.46875 8.6953125
noise_pred min, max: -7.84765625 6.25
 55%|██████████████████████████████████████████████████████████▎                                               | 11/20 [00:14<00:11,  1.30s/it]
Step 11, timestep 285:
latent_model_input_user min, max: -10.109375 9.4453125
noise_pred min, max: -8.46875 6.9296875
 60%|███████████████████████████████████████████████████████████████▌                                          | 12/20 [00:15<00:10,  1.30s/it]
Step 12, timestep 197:
latent_model_input_user min, max: -12.25 11.1015625
noise_pred min, max: -8.96875 8.1953125
 65%|████████████████████████████████████████████████████████████████████▉                                     | 13/20 [00:16<00:09,  1.30s/it]
Step 13, timestep 123:
latent_model_input_user min, max: -9.265625 8.2578125
noise_pred min, max: -8.75 9.4765625
 70%|██████████████████████████████████████████████████████████████████████████▏                               | 14/20 [00:18<00:07,  1.30s/it]
Step 14, timestep 69:
latent_model_input_user min, max: -9.1875 8.96875
noise_pred min, max: -8.359375 8.8125
 75%|███████████████████████████████████████████████████████████████████████████████▌                          | 15/20 [00:19<00:06,  1.30s/it]
Step 15, timestep 35:
latent_model_input_user min, max: -8.484375 8.71875
noise_pred min, max: -8.5859375 9.1640625
 80%|████████████████████████████████████████████████████████████████████████████████████▊                     | 16/20 [00:20<00:05,  1.30s/it]
Step 16, timestep 15:
latent_model_input_user min, max: -10.3671875 10.3203125
noise_pred min, max: -10.78125 9.4296875
 85%|██████████████████████████████████████████████████████████████████████████████████████████                | 17/20 [00:22<00:03,  1.30s/it]
Step 17, timestep 6:
latent_model_input_user min, max: -7.01171875 7.87109375
noise_pred min, max: -7.8984375 7.83984375
 90%|███████████████████████████████████████████████████████████████████████████████████████████████▍          | 18/20 [00:23<00:02,  1.30s/it]
Step 18, timestep 2:
latent_model_input_user min, max: -8.765625 8.6484375
noise_pred min, max: -9.5859375 6.75390625
 95%|████████████████████████████████████████████████████████████████████████████████████████████████████▋     | 19/20 [00:24<00:01,  1.30s/it]
Step 19, timestep 0:
latent_model_input_user min, max: -9.015625 9.0546875
noise_pred min, max: -9.015625 9.609375
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:26<00:00,  1.30s/it]
[DEBUG] Pipeline call finished.
(RPG) root@C.13784856:/home/kubig/Thumbnail_Image_Generation$ tmux capture-pane -pS -500 > /home/kubig/Thumbnail_Image_Generation/debug/hi

